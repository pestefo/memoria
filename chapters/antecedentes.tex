\chapter{Antecedentes}


\section{Marco Teórico}\seclabel{marco-teorico}

\subsection{Profiling}

\par El \emph{Perfil de Ejecución} (o \emph{Execution Profiling}) es una forma de análisis dinámico que consiste en monitorear un software durante su ejecución para posteriormente analizar los datos obtenidos. Entonces los \emph{Profilers} son herramientas para ayudar a los ingenieros de software a recolectar datos durante la ejecución y analizarlos, y así poder determinar de mejor manera en qué parte de un sistema de software se gasta el tiempo de ejecución.


\par Para la obtención de datos, existen varias técnicas de profiling. Los principales son profiling por instrumentación (o \emph{Instrumentation-based profiling}) y a través de muestras (o \emph{Sampling-based profiling}). En la primera, se inserta código de instrumentación antes (instrumentación estática) o durante la ejecución (instrumentación dinámica). En la primera se agrega el código justo antes de ejecutar y por tanto la sobrecarga (o overhead) es menor. Sin embargo, cualquier código generado dinámicamente no será instrumentado. En tanto, la técnica basada en muestras aproxima el tiempo que se gastó en un método de la aplicación deteniendo el programa y guardando la colección de metodos que fueron ejecutados.

\par Usualmente los \emph{profilers} son utilizados con motivo de optimizar un sistema de software en cierto aspecto. Los datos a obtener a través de la técnica de profiling dependen completamente del problema a optimizar. Es por esto que el profiler a utilizar debe permitir la recolección de los datos necesarios con una mínima sobrecarga (o \emph{overhead}) para conseguir datos fidedignos y a la vez representativos. 

\par La herramienta de profiling escogida para en este trabajo es el framework \emph{Spy}, que utiliza la técnica de profiling basado en instrumentación del tipo dinámico. Spy está diseñado para analizar softwares en el contexto de orientación a objetos ya que se permite obtener métricas contextuales como: el número de ejecuciones de un método, cantidad de objetos instanciados de una clase, etc. Para mayor información sobre este y su implementación en \emph{TestSurgeon} ver la \secref{impl-spy}. 

\par Algunas de las aplicaciones exitosas basadas en Spy son \emph{Hapao}\footnote{Hapao [en línea] \textless\url{http://hapao.dcc.uchile.cl}\textgreater  [Consulta: 12/08/2013]}: una herramienta de cobertura de test desarrollada por Vanessa Peña y Alexandre Bergel~\cite{bergel2012increasing}, que permite gráficamente ver la cobertura de las clases y métodos contrastando la cobertura de los últimos con el grado de complejidad de éstos; y \emph{Kai}\footnote{Object Profile. Kai Profiler [en línea] \textless\url{http://www.objectprofile.com/\#/pages/products/kai/overview.html}\textgreater  [Consulta: 12/08/2013] }: un profiler que permite optimizar la ejecución de un software indicando, por ejemplo, dónde combiene incrustar una estructura de \emph{caching} para evitar redundancia de cálculo. En la \figref{hapao.png} y \figref{kai.png} se muestran capturas de pantalla de Hapao y Kai respectivamente.

\fig{h}{0.7}{hapao.png}{Ejemplo de la visualización de la cobertura de test en Hapao}
\fig{h}{0.7}{kai.png}{Diagnóstico de ejecución entregado por Kai}

\clearpage
%==========
\subsection{SUnit framework}

\par \emph{SUnit} es un framework de testing que apoya la creación de tests automatizados. SUnit es la implementación en Smalltalk del framework xUnit\footnote{la \textbf{x} se reemplaza por el lenguaje de programación, por ejemplo: jUnit para Java.} que de hecho historicamente es la primera. Las cualidades que caracterizan xUnit son:

\begin{itemize}
\item Los tests son repetibles: Cada test se puede ejecutar cuantas veces se quiera.
\item Los tests se ejecutan si necesidad intervención humana: Se puede dejar los tests ``corriendo'' durante la noche si necesidad de intervenir en su ejecución.
\item Los tests cuentan una historia: un test cubre al menos una funcionalidad del sistema. Un test entonces es un escenario que cualquier desarrollador puede leer para entender dicha funcionalidad.
\end{itemize}

\par En SUnit los tests están contenidos en clases llamadas unit tests. Los unit tests son clases que extienden o heredan de {\tt TestCase}, una de las clases arquitectónicas del framework. Los unit tests suelen nombrarse con el sufijo {\tt Test}, por ejemplo {\tt MarioBrosTest}. Los tests o test methods, por su parte se nombran con {\tt test} como prefijo, ejemplo {\tt testMarioFallDownAndLoseOneLife}.


\subsubsection{Arquitectura de \emph{SUnit}}
\par Estructuralmente, SUnit se compone de cuatro clases: {\tt TestSuite}, {\tt TestCase}, {\tt TestResource} y {\tt TestResult} como se muestra en \figref{sunit.pdf}. 
\begin{description}
\item[{\tt TestCase}] representa un unit test o una familia de test que comparten el mismo contexto. Esta clase está pensada para ser extendida por clases concretas. El contexto se especifica en el método {\tt setUp} donde las variables de instancia de cada subclase de {\tt TestCase} se inicializan y definen el contexto en el cual se ejecutará cada test. También se define el método {\tt tearDown} que es responsable de limpiar los objetos que hayan sido inicializados y modificados durante la ejecución del test para que pueden ser reinicializados por el método {\tt setUp}. Tanto {\tt setUp} como {\tt tearDown} son métodos abstractos. Entonces cada test se ejecuta de la siguiente manera:

\[ \texttt{setUp} \rightarrow \texttt{testMarioFallDownAndLoseOneLife} \rightarrow \texttt{tearDown} \]

\item[{\tt TestSuite}] contiene una colección de unit tests. Una instancia de {\tt TestSuite} está compuesto por instancias de subclases de {\tt TestCase} y {\tt TestSuite}. Las clases {\tt TestSuite} y {\tt TestCase} forman el patrón composite. 

\item[{\tt TestResult}] representa los resultados de una ejecución de {\tt TestSuite}. Contiene el número de tests que pasadon, el número de tests que fallaron y el número de errores.

\item[{\tt TestResource}] representa un recurso que es usado por un test o un conjunto de tests. Un recurso se asocia con una clase hija de {\tt TestCase} y se ejecuta automáticamente antes de la ejecución de todos los tests y una sola vez.   
\end{description}

\fig{h}{0.7}{sunit.pdf}{Las clases que representan el núcleo de \emph{SUnit}}

\subsubsection{Estructura de un test}
\par La estructura de cada test puede dividirse en dos partes: \emph{fixture} (o escenario) y \emph{assertions} (o verificaciones). En el fixture del test se crean e inicializan los objetos del escenario y se les hace interactuar a través de mensajes. No se considera parte del fixture todos los objetos inicializados y las operaciones realizadas en la ejecución del método {\tt setup}, aunque sí afectan la interacción en el fixture. Posteriormente, en el test hay una serie de validaciones o verificaciones, donde se comprueba el estado de los objetos a través de métodos verificadores heredados desde {\tt TestCase} tales como: {\tt assert:} para verificar una afirmación, {\tt deny:} para negarla, {\tt should:raise:} para verificar que una excepción en particular se lanzó, y {\tt shouldnt:raise:} para asegurar que no se lanzó cierta excepción.

\par A modo de ejemplo se muestra un código que bien podría ser un test básico del popular juego \emph{Super Mario Bros.}, donde un plomero de traje rojo va pasando por escenarios llenos de trampas y dificultades para poder llegar al castillo donde supuestamente está su princesa y rescatarla. Una de las trampas más comunes y sencillas del juego son hoyos de profundidad infinita donde Mario puede caer y perder una vida. En el ejemplo, inicializamos la variable local {\tt mario} con una instancia de la clase {\tt MBMario} que por defecto tiene 3 vidas\footnote{En la plataforma NES, Mario parte con 3 vidas.}. Luego lo insertamos al juego junto con un escenario en el cual, luego de dar tres pasos debiera caer a un hoyo y perder una vida. En el juego no se acaba sino hasta que a Mario se le acaban las vidas y vemos el mensaje de ``Game Over''. 

\begin{codeWithLineNumbers}
<i>MarioBrosTest>>testMarioFallDownAndLoseOneLife</i>
	" *--* Fixture *--* "
	| game mario scene |
	game := MBGame new.
	mario := MBMario new. " todos saben que Mario parte con 3 vidas"
	scene := #(#floor #floor #hole #floor). " un escenario con un hoyo en la tercera posicion"

	game setScene: scene.
	game setPlayer: mario.

	" hagamos a mario avanzar en el escenario"
	mario moveOneStep. 
	mario moveOneStep.
	mario moveOneStep. 
		
	" *--* Assertions *--* "
	
	self assert: (game currentPosition = #hole) "mario cayo al hoyo" 
	self assert: mario isDead. "deberia haber muerto puesto que cayo al hoyo"
	self assert: (mario lives = 2). "mario ahora tiene 2 vidas"
	self deny: game isGameOver. 
	

\end{codeWithLineNumbers}

\newpage
%=======
\section{Trabajo Relacionado}\seclabel{trabajo-relacionado}

\par A continuación se presenta una revisión de la literatura sobre el problema de mantenibilidad de las pruebas de software, y una revisión también de las herramientas de cobertura de test que abordan tangencialmente el problema.

\subsection{Revisión de la literatura}
\par Luego de revisar la bibliografía existente, entre los problemas con mayor investigación está la cobertura de los test: cómo medir y mejorar la cobertura del software testeado para aumentar la confiabilidad en éste; y  el testeo por mutación: como mejorar la efectividad de los test realizando variaciones en el código testeado que representan bugs y comprobar que test definidos atrapan o detectan esas anomalías. Con respecto al problema de mantenibilidad, las estrategias encontradas en la literatura corresponden a priorización de tests y comparación de tests, pero en volumen mucho menor a los tópicos mencionados anteriormente.

\par La priorización de tests y el \emph{clustering} son acercamientos populares para manejar el gran costo de correr una cantidad considerable de tests. Shin Yoo \etal~\cite{yoo2009clustering} han presentado un mecanismo para reducir las comparaciones 1-a-1 agruparlos y de esa manera facilitar la tarea humana de priorizar tests. Ellos crearon clusters de tests agrupándolos por similitud de ejecución. Cada ejecución fue representada por una cadena binaria donde cada dígito (1 o 0) representa si cierto método fue ejecutado durante ésta. Entonces, la similitud de cobertura es medida a través de una distancia binaria.

\par Vengala \etal~\cite{Vang09a} crearon un mecanismo para usar análisis estático y dinámico para optimizar las actividades de testing tales como \emph{selección de test}, \emph{eliminación de redundancia} y \emph{priorización de test}. Aunque sus métricas siguen el mismo espíritu que \emph{TestSurgeon}, están definidas para un ambiente de programación procedural lo cual es incompatible en un ambiente orientado a objetos.

\par Greiler \etal~\cite{Grei12a,greiler2012test} aborda el problema de entendimiento de tests suites proponiendo un framework sobre correlación entre similitud de test (Test Similarity Correlator). Este último produce una traza de ejecución que caracteriza la ejecución de los test a través de cuatro tipos de eventos: ejecución de un test method, evento de set-up y tear-down, ejecución de un método y lanzamiento de excepción. El entendimiento de test suites se realiza comparando las trazas de ejecución.

\par El código de test, desde el punto de vista de un artefacto de software tiene múltiples usos. Uno de ellos es documentación~\cite{Kuhn08a}. Especialmente para desarrolladores nuevos que tienen que enfrentarse con sistemas legados, los tests son altamente relevantes como ejemplos de código. Siguiendo esta perspectiva, Lienhard \etal~\cite{Lien08a} proponen una herramienta que facilita la tarea de escribir nuevos tests para esos sistemas a través de una visualización que llaman \emph{Test Blueprint}, como una guía para el entendimiento de los tests y el mantenimiento del código base.  

\par Reichhart \etal~\cite{reichhart2007rule} presenta \emph{TestLint}, una herramienta para determinar la calidad de los tests. En su trabajo presenta una aplicación de esta sobre una gran cantidad de unit tests obtenidos desde distintos repositorios open source. Además entrega una gran lista de \emph{test smells} de los cuales uno es analizado por \emph{TestSurgeon} y corresponde al escenario descrito en la \secref{escenario2}.

\par De los autores encontrados, el que aborda con mayor profundidad el problema de mantenibilidad en los tests es Markus Gaelli. Dentro de su investigación~\cite{gaelli2006composing,gaelli2007composing} presenta un estudio de clasificación de unit test~\cite{gaelli2005towards} representado por un arbol binario cuyas ramas indican cumplimiento de un criterio de clasificación. En este arbol, las hojas son los distintos tipos de test: Test donde se llama un solo método exactamente una sola vez(~50\% del total de tests estudiados), Test donde se llama un solo método en varios escenarios distintos (15\%), etc. Luego, utilizando esta taxonomía crea un metamodelo para escribir tests a través de ejemplos (o tests de alto nivel) a través de composiciones de tests más pequeños y de bajo nivel. Este modelo facilita la detección de errores ya que al componer tests, un bug en el código base sólo rompería uno de los tests de los que conforman el test ejecutado. Gaelli en su trabajo pretende en su trabajo promover la reutilización de tests de bajo nivel para crear ejemplos de funcionamiento, o tests de alto nivel. De esta generar ejemplos que sirven tanto como pruebas de software y documentación ejecutable~\cite{gaelli2006modeling}, acercando la brecha entre las unidades de software y las pruebas unitarias.

\subsection{Revisión de herramientas de cobertura de test existentes}

\par La gran cantidad de herramientas disponibles ilustra la importancia de la cobertura de tests para los profesionales de la industria. Se realizó una revisión de muchos de ellos incluyendo Emma, EclEmma, JCover, GroboCodeCoverage, JCoverage, Parasoft JTest, Purity Plus, Semantic Designs, TCAT, Quilt, NoUnit, InsectJ, Hansel, Gretel, Jester, JVMDI Code Coverage, JBlanket, Coverlipse, Koalog, Cobertura, Mojo y Thucydides. En el \apref{review-test-coverage-tools} se presenta la lista con las URL donde se puede obtener cada una de ellas. Esta recolección de las herramientas más populares se realizó a través de motores de búsqueda en la web, directorios de proyectos open source, descripciones de entornos de desarrollo y publicaciones científicas entre los que están: maven\footnote{Apache Maven Project [en línea] \textless\url{http://maven.apache.org/}\textgreater [Consulta: 12/08/2013]}, sourceforge\footnote{Sourceforge [en línea] \textless\url{http://sourceforge.net/}\textgreater [Consulta: 12/08/2013]}, github\footnote{GitHub [en línea] \textless\url{http://github.com/}\textgreater [Consulta: 12/08/2013]}, bitbucket\footnote{BitBucket [en línea] \textless\url{http://bitbucket.org}\textgreater [Consulta: 12/08/2013]} y el Eclipse marketplace\footnote{Eclipse Marketplace [en línea] \textless\url{http://marketplace.eclipse.org/}\textgreater [Consulta: 12/08/2013]}. 

\par Dentro de las 22 herramientas de cobertura de test estudiadas, solo 3 soportan un análisis comparativo de cobertura de test. 
\par JCover indica las variaciones de cobertura mostrando un gráfico tipo \emph{scatterplot} puntos representando los pares \emph{(version del software, \% cobertura)}. Al parecer TCAT compara tests, sin embargo no hay suficiente información publicada, de hecho, su sitio web no ha sido actualizado desde el año 2008. Jester realiza mutación a los tests para verificar cuán capaz es el unit test de capturar variaciones en el código base.

